% \documentclass{amsart}

\documentclass{article}
% \usepackage{blindtext}
\usepackage[a4paper, total={6in, 10in}]{geometry}

\title{Dynamatte: A dynamic matting method to generate in scene video fusion}
\author{Richa Mishra \\
\small\texttt{richmis@andrew.cmu.edu}
\and
Achleshwar Luthra \\
\small\texttt{achleshl@andrew.cmu.edu}
\and
Christopher Klammer \\
\small\texttt{cklammer@andrew.cmu.edu}}

% \date{\today}


\usepackage{graphicx}
\usepackage{biblatex}
\usepackage{xcolor}
\usepackage{parskip}
\addbibresource{midterm.bib}

\begin{document}
\maketitle

\section{Motivation}

Imagine standing on a sandy beach, the wind whipping through your hair as you gaze out at the horizon. The sun, a giant ball of fire, seems to kiss the ocean as it sinks lower and lower, casting a warm glow across everything it touches. You realize that this moment is too perfect not to capture. and you begin to record the frothy waves lapping at the shore, the gulls circling overhead, and the distant hum of laughter and conversation from the people around you.

As you pan the camera to the horizon, you wish there was something even more to accentuate the colors in the sky. Maybe a rainbow kite, soaring high above the waves, and then, just beyond it, you imagine spotting a surfer riding a wave, his silhouette outlined against the fiery sky. You think of the lost potential $-$ the combination of the stunning sunset, the playful kite, and the daring surfer would have been nothing short of magical. Watching the footage back, you feel awe at the world's beauty but dissatisfied with the limitations of current technology to capture vibrant moments.

In video editing, much care and attention must be dedicated towards creating seamless and temporally consistent video with synthetically inserted objects or elements. These efforts consume an obscene amount of time to execute and perfect in post production. However, what if that burden could be ameliorated with a method that could add dynamic objects with photorealistic qualities into a live scene? 

Overall, the impact this project could have on video editing cannot be understated. The benefits include a streamlined VFX process to obtain clean foreground and background mattes. Furthermore, scene complications such as dust, reflections, and shadows are challenging cases that our neural network will be able to handle in seconds instead of hours.

\section{Prior Work}
Many works in the past have looked at the task of matte creation. Lu et al.\cite{lu2021} propose Omnimatte, a method to associate pixels in the screen with masked objects in a given scene. This method allows complex effects such as shadows, reflections, smoke, and water ripples to appear in the Omnimatte. This method takes videos as input to train a 2D U-Net that processes the video frame by frame. Each object is processed through a different layer in the model with dense optical flow fields calculated to maintain temporal consistency in the RGBA output. 

Sengupta et al.\cite{BMSengupta20} capture image and background pairs to estimate an alpha matte and foreground in order to place humans in scenes with novel backgrounds. It uses context switching to combine different cues and a self supervised adversarial loss to take a blended image with the novel background, generating realistic images. This method requires an additional photo of the background without the subject at the time of capture which limits us from applying this method to a random image where there is unavailability of subject-free background and it also demands for a certain amount of foresight during the process of data collection. 

Our project idea aims to overcome this limitation\cite{BMSengupta20} by taking inspiration from\cite{lu2021} that just needs a video and segmentation masks of objects of interest, and is capable of estimating omnimattes $-$ an alpha matte and color image that includes the subject along with all its time-varying scene elements. 

% \textcolor{red}{(Third potential paper: Layered Neural Atlases)}

\section{The Idea}

\subsection{Using Omnimattes for Video-in-Video Insertion}
As stated before, Omnimatte is a method that has a purpose of taking an input mask for a given object and to return an omnimatte which contains color and opacity along with an optical flow field. Our solution aims to use omnimattes extracted from a video sequence and insert the omnimatte seamlessly into a different video sequence. More formally, for a given video sequence $\mathcal{V}$, we process the video sequence an extract out omnimattes for object $i$ at each time $t$, homography $H$, mask $M$ and flow $F$ as $O(I_t, H_t, M_t^i, F_t^i)$ = \{$\alpha_t^i, C_t^i, \hat{F_t^i}$\} where $\alpha_t^i$ represents the opacity for the object at a given frame, $C_t^i$ represents the color, and $\hat{F_t^i}$ is the object's flow.

\subsection{Our Contributions}
We plan to take the result of Omnimatte and seamlessly take one to many omnimattes and seamlessly insert them into an existing video. By doing this, we plan to use existing videos and allow dynamic objects to be added to the videos. The main challenge of this work will be to seamlessly perform domain transfer and conform to noise and background objects.

% Let's put this in the related work
% In Background Matting },  Our project idea aims to overcome this limitation by taking inspiration from  that just needs a video and segmentation masks of objects of interest, and is capable of estimating omnimattes â€” an alpha matte and color image that includes the subject along with all its time-varying scene elements.

Our contribution will look to have two stages. Stage one will include extracting the omnimatte(s) from a video sequence. Stage two will include taking the omnimatte and inserting it into sequence with our network. Our network will have to make sure the omnimatte is temporally consistent, is blended with the background, is inserted into a logical location, and accounts for style transfer for the target sequence.

\subsection{Loss}
We enforce inserting objects logically in locations that are readily available in the scene. Thus, we aim to

\subsection{Insertion Gradient Loss}
We aim to enforce a smooth insertion between the object and the background. In order to enforce this, we first plan to take the omnimatte mask as an input. For this region, we plan to attempt to match  

\subsubsection{Style Loss}
The style of an image usually refers to the texture and other low level features of an image. We plan to use the features from the output of our backbone network. We may experiment with using earlier outputs from layers as they will likely have learned more low-level features.

We compute the \textbf{gram matrix} for all $C$ feature vectors of the feature maps for the inserted object and the target sequence. We can then calculate the style loss as a mean squared error of the object and target's gram matricies.

Most of the time, the size of the object will not equal the size of the target scene. However, we instead choose to look at the gram matrix of the image before and after insertion of the target in order to measure the information gain (or loss) of our synthetic insertion. To clarify notation, we refer to $G^{obj}$ as the gram matrix after the object is inserted and $G^{tgt}$ to signify the gram matrix of the target image before the object is inserted.

$$G_{i, j} = F_i \cdot F_j$$
$$\mathcal{L}_{style} = \frac{1}{N_{obj} N_{tgt}} \sum_i \sum_j (G_{i, j}^{tgt} - G_{i, j}^{obj})^2$$

Where $F$ is a list of flattened feature vectors for all $C$ feature maps of the backbone output. $N_{obj}$ and $N_{tgt}$ are the number of entries in the gram matrix of the object and target respectively.

\subsubsection{Temporal Consistency Loss}
A novel contribution and main challenge of this work is to maintain temporal consistency. Intuitively, when inserting an object into a video, we want to see the following:

\begin{enumerate}
    \item Cohesive and smooth motion frame to frame
    \item Possible occlusion if passing by another object in the scene
    \item Disappearance of the object when at the edge of the frame
\end{enumerate}

\section{Experiments and Timeline}
\begin{center}
\begin{tabular}{|| p{2cm} | p{12cm} ||}
    \hline
    \textbf{Date} & \textbf{Milestone} \\
    \hline
    Mid March & Start development of baselines \\
    \hline
    Early April & Baseline results finished, mid-term report write-up, start development of proposed solution \\
    \hline
    Mid April & Progress on proposed solution, some initial results with issues documented \\
    \hline
    End April & Finished attempt of proposed solution with qualitative results, lessons learned, and limitations\\
    \hline
    Early May & Written final report \\
    \hline
\end{tabular}
\end{center}

\section{Baselines}
Our baselines 


Old:
As a baseline, we plan to manually extract omnimattes and attempt to use an extremely simple "lazy" cut and paste as our first baseline. This baseline will not be temporally consistent but will place the omnimattes in a "logical" part in the image. We then will perform frame-by-frame poisson image blending to create a blended video. We then will either use the famous GP-GAN\cite{10.1145/3343031.3350944} or Background Matting \cite{BMSengupta20} to create the blended video that will be more temporally consistent.

\section{Results}


\printbibliography
\end{document}
