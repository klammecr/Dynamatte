% \documentclass{amsart}

\documentclass{article}
% \usepackage{blindtext}
\usepackage[a4paper, total={6in, 10in}]{geometry}

\title{Dynamatte: A dynamic matting method to generate in scene video fusion}
\author{Richa Mishra \\
\small\texttt{richmis@andrew.cmu.edu}
\and
Achleshwar Luthra \\
\small\texttt{achleshl@andrew.cmu.edu}
\and
Christopher Klammer \\
\small\texttt{cklammer@andrew.cmu.edu}}

% \date{\today}


\usepackage{graphicx}
\usepackage{biblatex}
\usepackage{xcolor}
\usepackage{parskip}
\addbibresource{proposal.bib}

\begin{document}
\maketitle
% \begin{abstract}
%  Put an abstract here when you're ready.
% \end{abstract}

% \tableofcontents

\section{Motivation}

Imagine standing on a sandy beach, the wind whipping through your hair as you gaze out at the horizon. The sun, a giant ball of fire, seems to kiss the ocean as it sinks lower and lower, casting a warm glow across everything it touches. You realize that this moment is too perfect not to capture. and you begin to record the frothy waves lapping at the shore, the gulls circling overhead, and the distant hum of laughter and conversation from the people around you

As you pan the camera to the horizon, you wish there was something even more to accentuate the colors in the sky. Maybe a rainbow kite, soaring high above the waves, and then, just beyond it, you imagine spotting a surfer riding a wave, his silhouette outlined against the fiery sky. You think of the lost potential, the combination of the stunning sunset, the playful kite, and the daring surfer would have been nothing short of magical. Watching the footage back, you feel awe at the world's beauty but dissatisfied with the limitations of current technology to capture vibrant moments.

In video editing, much care and attention must be dedicated towards creating seamless and temporally consistent video with synthetically inserted objects or elements. These efforts consume an obscene amount of time to execute and perfect in post production. However, what if that burden could be ameliorated with a method that could add dynamic objects with photorealistic qualities into a live scene? 

Overall, the impact this project could have on video editing cannot be understated. The benefits include a streamlined VGX process to obtain clean foreground and background mattes. Furthermore, scene complications such as dust, reflections, and shadows are challenging cases that our neural network will be able to handle in seconds instead of hours.

% Imagine standing on a sandy beach, the wind whipping through your hair as you gaze out at the horizon. The sky is alive with an explosion of hues - pinks, oranges, yellows, and reds - that dance and meld together in a symphony of color. The sun, a giant ball of fire, seems to kiss the ocean as it sinks lower and lower, casting a warm glow across everything it touches.

% As you take out your camera, you realize that this moment is too perfect not to capture. You begin to record, the lens capturing every detail - the frothy waves lapping at the shore, the gulls circling overhead, and the distant hum of laughter and conversation from the people around you.

% But then, as you pan the camera to the horizon, you wish there was something even more to accentuate the colors in the sky. Maybe a rainbow kite, soaring high above the waves, its vibrant colors a stark contrast to the soft pinks and oranges of the sunset. And then, just beyond it, you imagine spotting a surfer riding a wave, his silhouette outlined against the fiery sky.

% You think of the lost potential, the combination of the stunning sunset, the playful kite, and the daring surfer would have been nothing short of magical. As you watch the footage back, you can't help but feel a sense of awe at the beauty of the world around you while wildly dissastified with the limitations of current technology and you couldn't make your picturesque moment that much more vibrant. 

\section{Prior Work}
Many works have have looked at the task of matte creation. Sengupta et al.\cite{BMSengupta20} capture image and background pairs to estimate an alpha matte and foreground in order to place humans in scenes with novel backgrounds. This method requires an additional photo of the background without the subject at the time of capture which limits us from applying this method to a random image where there is unavailability of subject-free background and it also demands for a certain amount of foresight during the process of data collection. It uses context switching to combine different cues and a self supervised adversarial loss to to take a blended image with the novel background, generating realistic images.

Lu et al.\cite{lu2021} propose Omnimatte, a method to associate pixels in the screen with masked objects in a given scene. This method allows complex effects such as shadows, reflections, smoke, and water ripples to appear in the Omnimatte. This method takes videos as input to train a 2D U-Net that processes the video frame by frame. Each object is processed through a different layer in the model with dense optical flow fields calculated to maintain temporal consistency in the RGBA output.

Background Matting\cite{BMSengupta20} requires an additional photo of the background without the subject at the time of capture which limits us from applying this method to a random image where there is unavailability of subject-free background and it also demands for a certain amount of foresight during the process of data collection. Our project idea aims to overcome this limitation by taking inspiration from\cite{lu2021} that just needs a video and segmentation masks of objects of interest, and is capable of estimating omnimattes $-$ an alpha matte and color image that includes the subject along with all its time-varying scene elements. 

% \textcolor{red}{(Third potential paper: Layered Neural Atlases)}

\section{The Idea}
We plan to take the result of Omnimatte and seamlessly take one to many omnimattes and seamlessly insert them into an existing video. By doing this, we plan to use existing videos and allow dynamic objects to be added to the videos. The main challenge of this work will be to seamlessly perform domain transfer and conform things like noise and background to the objects.

% Let's put this in the related work
% In Background Matting },  Our project idea aims to overcome this limitation by taking inspiration from  that just needs a video and segmentation masks of objects of interest, and is capable of estimating omnimattes â€” an alpha matte and color image that includes the subject along with all its time-varying scene elements.

Our contribution will look to have two stages. Stage one will include extracting the omnimatte(s) from a video sequence. Stage two will include taking the omnimatte and inserting it into to sequence with our network. Our network will have to make sure the omnimatte is temporally consistent, is blended with the background, is inserted into a logical location, and accounts for style transfer for the target sequence.

\section{Experiments and Timeline}
\begin{center}
\begin{tabular}{|| p{2cm} | p{12cm} ||}
    \hline
    \textbf{Date} & \textbf{Milestone} \\
    \hline
    Mid March & Start development of baselines \\
    \hline
    Early April & Baseline results finished, mid-term report write-up, start development of proposed solution \\
    \hline
    Mid April & Progress on proposed solution, some initial results with issues documented \\
    \hline
    End April & Finished attempt of proposed solution with qualitative results, lessons learned, and limitations\\
    \hline
    Early May & Written final report \\
    \hline
\end{tabular}
\end{center}

\section{Baselines}
As a baseline, we plan to manually extract omnimattes and attempt to use an extremely simple "lazy" cut and paste as our first baseline. We then will perform frame-by-frame poisson image blending to create a blended video. We then will either use the famous GP-GAN paper or Background Matting \cite{BMSengupta20} to create the blended video.


\printbibliography
\end{document}
